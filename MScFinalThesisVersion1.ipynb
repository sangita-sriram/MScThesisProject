{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwordsi = set(STOPWORDS)\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import eli5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy import displacy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "\n",
    "#Dropping NA values only from text column\n",
    "train = train.dropna(subset=['text'])\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting text to lower case \n",
    "train['text'] = train['text'].apply(lambda word: \" \".join(y.lower() for y in word.split()))\n",
    "\n",
    "#removing duplicate records\n",
    "train=train.drop_duplicates(subset='text', keep=False)\n",
    "\n",
    "#Remove punctuations\n",
    "train['text'] = train['text'].apply(lambda word: re.sub(r'[^\\w\\s.]', '', word))\n",
    "train['text'] = train['text'].apply(lambda word: re.sub(r'[-]', ' ', word))\n",
    "train['text'] = train['text'].apply(lambda word: re.sub(r'[_]', '', word))\n",
    "\n",
    "#Removing any numbers and alphanumeric texts\n",
    "train['text'] = train['text'].apply(lambda word: re.sub(r'[0-9]', '', word))\n",
    "\n",
    "#Retains only English alphabetical words\n",
    "train['text'] = [' '.join(w for w in p.split() if re.match(r'[a-z]', w)) for p in train['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition replacement\n",
    "#Takes 40 minutes to run\n",
    "def tag_ne(content):\n",
    "    doc = nlp(content)\n",
    "    text = doc.text\n",
    "    for ent in doc.ents:\n",
    "        text = re.sub(ent.text, ent.label_, text)\n",
    "    return text\n",
    "\n",
    "print(datetime.now().time())\n",
    "train['text']=train['text'].apply(tag_ne)\n",
    "print(datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes stop words\n",
    "stop_words = set(STOPWORDS)\n",
    "train['text']=train['text'].apply(lambda word: \" \".join(w for w in nltk.word_tokenize(word) if w not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Removing non-english words\n",
    "\n",
    "###commented because it removes words having fullstop next to it, hence not able to split them by sentences,/\n",
    "###as sentences require full stop after them\n",
    "\n",
    "###Can perform this step after splitting the text into sentences\n",
    "\n",
    "#englishwords = set(nltk.corpus.words.words())\n",
    "#train['text'] = train['text'].apply(lambda word: \" \".join(w for w in nltk.wordpunct_tokenize(word) if w in englishwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes words that have length less than 3\n",
    "\n",
    "#train['text']=train['text'].apply(lambda word: \" \".join(w for w in word.split() if re.match(r'\\b\\w{3,}\\b', w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To perform stemming\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "train['text'] = train['text'].apply(lambda word: \" \".join([stemmer.stem(w) for w in word.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the text into reliable and fake, to distinguish between the two\n",
    "\n",
    "reliable = train[train['label']==0]\n",
    "fake = train[train['label']==1]\n",
    "\n",
    "#counting the number of words in text column\n",
    "reliable[\"num_words\"] = reliable[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "fake[\"num_words\"] = fake[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "print(\"fake mean number of words:\", fake.num_words.mean())\n",
    "print(\"reliable mean number of words:\", reliable.num_words.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud for reliable 'text' column\n",
    "\n",
    "mpl.rcParams['font.size']=12\n",
    "mpl.rcParams['savefig.dpi']=100\n",
    "mpl.rcParams['figure.subplot.bottom']=.1\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wordcloudR = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stopwords,\n",
    "                          max_words=50,\n",
    "                          max_font_size=80, \n",
    "                          random_state=42,\n",
    "                         ).generate(str(reliable['text']))\n",
    " \n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloudR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word cloud for fake 'text' column\n",
    "\n",
    "mpl.rcParams['font.size']=12\n",
    "mpl.rcParams['savefig.dpi']=100\n",
    "mpl.rcParams['figure.subplot.bottom']=.1\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "wordcloudFake = WordCloud(\n",
    "                          background_color='white',\n",
    "                          stopwords=stopwords,\n",
    "                          max_words=50,\n",
    "                          max_font_size=80, \n",
    "                          random_state=42\n",
    "                         ).generate(str(fake['text']))\n",
    " \n",
    "fig = plt.figure(1)\n",
    "plt.imshow(wordcloudFake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom function for ngram generation ##\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "## custom function for horizontal bar chart ##\n",
    "def horizontal_bar_chart(df, color):\n",
    "    trace = go.Bar(\n",
    "        y=df[\"word\"].values[::-1],\n",
    "        x=df[\"wordcount\"].values[::-1],\n",
    "        showlegend=False,\n",
    "        orientation = 'h',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        )\n",
    "    )\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in reliable[\"text\"]:\n",
    "    for word in generate_ngrams(sent,1):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "reliable1g = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in fake.text:\n",
    "    for word in generate_ngrams(sent,1):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "fake1g = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Reliable text\", \n",
    "                                          \"Fake text\"])\n",
    "fig.append_trace(reliable1g, 1, 1)\n",
    "fig.append_trace(fake1g, 1, 2)\n",
    "fig['layout'].update(height=500, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Frequent 1-gram Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams chart\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in reliable[\"text\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "reliable2g = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n",
    "\n",
    "\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in fake['text']:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "fake2g = horizontal_bar_chart(fd_sorted.head(20), 'orange')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n",
    "                          subplot_titles=[\"Reliable text\", \n",
    "                                          \"Fake text\"])\n",
    "fig.append_trace(reliable2g, 1, 1)\n",
    "fig.append_trace(fake2g, 1, 2)\n",
    "fig['layout'].update(height=500, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Frequent Bigram Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tfidf vectors \n",
    "tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train['text'].values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the word features\n",
    "\n",
    "print(tfidf_vec.get_feature_names())\n",
    "\n",
    "print(\"shape:\", train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing 'label' column values to train_y\n",
    "train_y=train['label'].values\n",
    "\n",
    "#Split into training and test set in 70:30 ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tfidf, train_y, random_state=0,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC()\n",
    "\n",
    "lsvc.fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of LSVC classifier on training set: {0}'.format(lsvc.score(X_train, y_train)))\n",
    "predictions = lsvc.predict(X_test)\n",
    "\n",
    "print('Accuracy of LSVC classifier on test set:')\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(lsvc, vec=tfidf_vec, top=20, feature_filter=lambda x: x != '<BIAS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier(max_depth=3)\n",
    "dt = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import graphviz \n",
    "#dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "#graph = graphviz.Source(dot_data) \n",
    "#graph.render(\"dataLatest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(dt.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(dt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(dt, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
